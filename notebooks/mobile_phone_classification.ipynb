{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt # It provides an implicit, MATLAB-like, way of plotting.\n",
    "\n",
    "from sklearn.model_selection import train_test_split #split data\n",
    "\n",
    "# to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# build the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # Confusion matrix with clear numbers inside boxes\n",
    "\n",
    "import sklearn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = '/home/alo-vebrisatriadi/Documents/data-engineering/learning/mlops/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis (EDA)** is the process of examining and visualizing a dataset to gain insights, understand its structure, and uncover patterns before any modeling. It helps in preparing the data for machine learning by identifying key trends, relationships, and potential issues like missing values or outliers.\n",
    "\n",
    "*Key Points*:\n",
    "- **Summarize Data**: Understand basic statistics (mean, median, range) and data types.\n",
    "- **Visualize**: Use charts like histograms, scatter plots, and heatmaps to explore distributions and relationships.\n",
    "- **Detect Issues**: Spot missing data, outliers, and anomalies that might affect modeling.\n",
    "- **Guide Feature Engineering**: Helps decide which features are important and need further transformation.\n",
    "EDA ensures that your data is well-understood and clean before applying any models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PROJECT_PATH + 'data/mobile_phone_classification/train.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show shape \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplication\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some information about out data\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here ... Take a look to columns meaning, I can note that we have px_width, px_height which reference to high and width of mobile with pixels, and we have sc_h, sc_w which reference to high and width of mobile with inch ..\n",
    "\n",
    "We need to make sure that are the values directly related ??.\n",
    "\n",
    "I mean that if px_width, px_height are resolutions in pixels and sc_w, sc_h are screen dimensions in inches, then they should have a mathematical relationship through the PPI (pixels per inch) ratio.\n",
    "\n",
    "We could check if a constant factor links these pairs of columns, If the ratios are consistent, it means the values are just different representations of the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['px_ratio_width'] = df['px_width'] / df['sc_w']\n",
    "df['px_ratio_height'] = df['px_height'] / df['sc_h']\n",
    "print(df[['px_ratio_width', 'px_ratio_height']].sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presence of inf and NaN values in px_ratio_width indicates that there are issues, likely due to division by zero or very small numbers in the sc_w column. These values would not appear if the ratios were consistent.\n",
    "\n",
    "These results imply that px_width, px_height, sc_w, and sc_h are not simply scaled versions of each other. They likely represent different aspects of the phone's characteristics (e.g., pixel density might vary significantly across different phones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove (px_ratio_height, px_ratio_width) columns from data\n",
    "df.drop(['px_ratio_height', 'px_ratio_width'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This previous function provides a set of summary statistics that help you better understand the distribution of data, including:\n",
    "\n",
    "- Count: the number of non-empty values in each column.\n",
    "- Arithmetic mean( mean): the average of the values of the numerical column.\n",
    "- Standard deviation (std): a measure of the spread of data about the arithmetic mean.\n",
    "- Minimum (min): the smallest value in the column.\n",
    "- First quarter (25%): the value that separates the smallest 25% of the data.\n",
    "- Median (50%): the value that separates the lower half of the data from the upper half (the same arithmetic mean in the case of a symmetric distribution).\n",
    "- Third quarter (75%): the value that separates the largest 25% of the data.\n",
    "- Upper limit (max): the largest value in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Heatmap, this will help you understand relationships between numerical variables.\n",
    "plt.figure(figsize=(18, 16))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Visualization\n",
    "# Understand which features in the dataset are most important in determining the value of \"price_range\".\n",
    "df.drop('price_range', axis=1).corrwith(df.price_range).plot(kind='bar', grid=True, figsize=(15, 5)\n",
    "                                                             ,title=\"Correlation with price_range\", color=\"Blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Features, Use histograms to check the distribution of numerical features.\n",
    "df.hist(figsize=(25, 23))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot for Feature Relationships, A pairplot can give insights into relationships between features.\n",
    "plt.figure(figsize=(25, 23))\n",
    "sns.pairplot(data = df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distribution_plot(df_train, continuous_features):\n",
    "    data = df_train[continuous_features].copy()\n",
    "\n",
    "    # create subplots\n",
    "    fig, axes = plt.subplots(nrows=len(data.columns)//2, ncols=2, figsize=(20, 40))\n",
    "    fig.subplots_adjust(hspace=0.7)\n",
    "\n",
    "    # set fontdict\n",
    "    font = {'family': 'serif',\n",
    "            'color': 'darkred',\n",
    "            'weight': 'normal',\n",
    "            'size': 16,\n",
    "            }\n",
    "    \n",
    "    for ax, feature in zip(axes.flatten(), data.columns):\n",
    "        feature_mean = data[feature].mean()\n",
    "        feature_median = data[feature].median()\n",
    "        feature_mode = data[feature].mode().values[0]\n",
    "        sns.distplot(data[feature], ax=ax)\n",
    "        ax.set_title(f'Analysis of {feature}', fontdict=font)\n",
    "        ax.axvline(feature_mean, color='r', linestyle='--', label=\"Mean\")\n",
    "        ax.axvline(feature_median, color='g', linestyle='-', label=\"Median\")\n",
    "        ax.axvline(feature_mode, color='b', linestyle='-', label=\"Mean\")\n",
    "        ax.legend()\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = ['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',\n",
    "       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',\n",
    "       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',\n",
    "       'touch_screen', 'wifi', 'price_range']\n",
    "\n",
    "generate_distribution_plot(df, continuous_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show boxplot for all columns\n",
    "for i in df.columns:\n",
    "    sns.boxplot(x=i, data = df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no null value which need to handle or values need to scale .. so we will go to splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('price_range', axis=1)\n",
    "y = df['price_range']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest** is an ensemble learning method that improves predictive accuracy by combining multiple decision trees. Each tree is trained on a random subset of data and features, and the final prediction is made by aggregating the results from all trees. This method reduces overfitting, handles large datasets well, and provides insights into feature importance.\n",
    "\n",
    "***Key Points***:\n",
    "- **Ensemble Method**: Combines predictions from multiple trees.\n",
    "- **Decision Trees**: Individual trees are trained on different data subsets.\n",
    "- **Bagging**: Reduces overfitting by using different samples of data.\n",
    "- **Feature Randomness**: Uses a subset of features for each tree.\n",
    "\n",
    "***Benefits***:\n",
    "- **Enhanced Accuracy**: Reduces errors compared to single decision trees.\n",
    "- **Versatile**: Suitable for both classification and regression tasks.\n",
    "- **Feature Importance**: Highlights which features are most influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'random_state': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Forest model\n",
    "rf_model = RandomForestClassifier(**params)\n",
    "rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [None, 8, 16, 32],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['entropy', 'gini']\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(**params)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test set score:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Confusion Matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred,\n",
    "                                        cmap = plt.cm.Blues,\n",
    "                                        normalize=None,\n",
    "                                        display_labels=['0', '1', '2', '3'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Learning curve calculation\n",
    "train_sizes, train_scores, test_scores = learning_curve(RandomForestClassifier(random_state=42),\n",
    "                                                        X, y, cv=5,\n",
    "                                                        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                                                        scoring='accuracy', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average accuracy and standard deviation for each training step.\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curve drawing\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label=\"Train Accuracy\", color=\"blue\", marker='o')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"blue\", alpha=0.2)\n",
    "\n",
    "plt.plot(train_sizes, test_mean, label=\"Test Accuracy\", color=\"green\", marker='o')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"green\", alpha=0.2)\n",
    "\n",
    "plt.title('Learning Curve for Random Forest')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_model, 'random_forest_model_joblib.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "loaded_model = joblib.load('random_forest_model_joblib.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save model using pickle\n",
    "with open(PROJECT_PATH + 'models/mobile_phone_classification.pkl', 'wb') as file:\n",
    "    pickle.dump(rf_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model using pickle\n",
    "with open(PROJECT_PATH + 'models/mobile_phone_classification.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save model using pickle with a different extension\n",
    "with open('random_forest_model_pickle.model', 'wb') as file:\n",
    "    pickle.dump(rf_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model using pickle from a different extension\n",
    "with open('random_forest_model_pickle.model', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
